{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Features\n",
    "\n",
    "On the previous class, we looked on how to create a linear regression pipeline using a single feature. Now, we will see how to apply a linear regression with multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tests, we will use the same dataset from the last lecture, the Boston house pricing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/BostonHousing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "msk = np.random.rand(len(data)) < 0.8 # Create a mask with random indexes\n",
    "\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "\n",
    "print(len(data), len(train), len(test), len(train) + len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a list of features and target for the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built', 'condition']\n",
    "target = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to look at is, instead of an analytical solution, we will need to find an approximate solution.\n",
    "\n",
    "The features can be represented by a set of arrays $x_{i}[j]$, where $j$ is the feature and $i$ is a feature element. The \"linear\" equation (now a multi-dimensional plane) will have the intercept and a coefficient for each feature, and is written as:\n",
    "\n",
    "$$y_{i}^{,}(x_i) = w_0*x_{i}[0] + w_{1}*x_{i}[1] + w_{2}*x_{i}[2] + \\dots + w_{d}*x_{i}[d] = \\sum_{j=0}^{d}w_j*x_{i}[j]$$\n",
    "\n",
    "where $x_i[0]=1$. We can use matrix notation and use the dot product of the parameters $\\textbf{w}$ with the features $\\textbf{x}_i$:\n",
    "\n",
    "$$y_{i}^{,}(\\textbf{x}_i) = \\textbf{w}^{T}\\textbf{x}_i$$\n",
    "\n",
    "As for the linear regression with a single feature, the solution for multiple features come from the minimization of the ***Residual Sum of Squares*** (RSS):\n",
    "\n",
    "$$RSS(\\textbf{w}) = \\sum_{i=1}^{N}(y_i - y_{i}^{,})^2 = \\sum_{i=1}^{N}(y_i - \\textbf{w}^{T}\\textbf{x}_i)^2 = (\\textbf{y} - \\textbf{w}^{T}\\textbf{x})^{T}(\\textbf{y} - \\textbf{w}^{T}\\textbf{x})$$\n",
    "\n",
    "The gradient $\\Phi$ (the derivative of RSS relative to $\\textbf{w}$) is:\n",
    "\n",
    "$$\\Phi = -2\\textbf{x}^{T}(\\textbf{y} - \\textbf{w}^{T}\\textbf{x})$$\n",
    "\n",
    "In others words, we just need to take the dot product between the features and the residuals (the difference between the predictions and the measured data) and multiply by 2. The gradient is used to update the parameters $\\textbf{w}$ iteratively in a way to minimize the errors (set the gradient to zero). In the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) method, for the n-th iteration, we end up with the following equation:\n",
    "\n",
    "$$\\textbf{w}_{n+1} = \\textbf{w}_{n} - \\alpha\\Phi = \\textbf{w}_{n} + 2{\\alpha}\\textbf{x}^{T}(\\textbf{y} - \\textbf{w}^{T}_{n}\\textbf{x})$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, a scalar that helps the minimization to converge faster to the minimum point. The choice of the step length shows to be trick, as a large value wil lead to minimization to diverge as a too small value will diverge slowly.\n",
    "\n",
    "Now we shall be able to compute the best set of parameters given a set of features and the output target. But to compute this, first we need to convert the data in the sframe format (the Turicreate dataframe) to a numpy numerical matrix. We will create a series of functions to combine in a final one that will calculate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the operations, let's convert the columns of the dataframe to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(df, features, output):\n",
    "    # First, create a column with the feature x_0 = 1\n",
    "    df['constant'] = 1\n",
    "\n",
    "    # Include the w_0 in the features list in the first column\n",
    "    features = ['constant'] + features \n",
    "    \n",
    "    # From the data, pick only desired features and convert to numpy array\n",
    "    feature_matrix = df[features].values\n",
    "    \n",
    "    # Doing the same for the output\n",
    "    output_array = df[output].values\n",
    "    \n",
    "    # Return the features and output target as numerical matrices\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(example_features, example_output) = get_numpy_data(data, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
    "print(example_features[0,:])\n",
    "print(example_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make our life easier, we can use the fact that the predictions are simply the **dot product** of the **weights** with the **features**, to create a function to compute the prediction given the matrix of the features and the array of parameters (weights). For that, we will the *Numpy* function [dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # The predictions are the dot product of the features and weights (parameters)\n",
    "    predictions = ### YOUR CODE HERE ###\n",
    "    \n",
    "    # Return the predictions\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First computing manually\n",
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "my_features = example_features[0,] # we'll use the first data point\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "\n",
    "# Now using our function\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "\n",
    "print(predicted_value)\n",
    "print(test_predictions[0])\n",
    "print(test_predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function that, by giving the residuals and features, it returns the gradient $\\Phi$ (the derivative of RSS relative to $\\textbf{w}$):\n",
    "\n",
    "$$\\Phi = -2\\textbf{x}^{T}(\\textbf{y} - \\textbf{w}^{T}\\textbf{x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(residuals, feature):\n",
    "    # Using the equation of the derivative, do the dot product between the features and residuals\n",
    "    derivative = ### YOUR CODE HERE ###\n",
    "    \n",
    "    # Return the derivative\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only on feature\n",
    "(example_features, example_output) = get_numpy_data(data, ['sqft_living'], 'price') \n",
    "\n",
    "# Making the parameters = 0 so the derivative should be\n",
    "# equal the sum of the output. This is an easy way to\n",
    "# evaluate our function\n",
    "my_weights = np.array([0., 0.])\n",
    "\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "\n",
    "residuals = test_predictions - example_output\n",
    "\n",
    "# Let's compute the derivative with respect to 'constant',\n",
    "# the \":\" indicates \"all rows\"\n",
    "features = example_features[:,0] \n",
    "\n",
    "derivative = feature_derivative(residuals, features)\n",
    "\n",
    "print(derivative)\n",
    "print(-np.sum(example_output)*2) # should be the same as derivative\n",
    "print(derivative - (-np.sum(example_output)*2)) # should be ZERO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now, let's head to the to the gradient descent method. \n",
    "\n",
    "First, we need to define a stoping criteria (tolerace), a value that stops the loop if the gradient is smaller than it. Also, it needs the initial parameters (weights) guess, and the step length (set as constant for all the iterations) as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    iteration = 0\n",
    "    \n",
    "    while not converged:\n",
    "        # Compitung predictions with current weights\n",
    "        predictions = predict_output(feature_matrix, weights) \n",
    "        \n",
    "        # Computing current residuals\n",
    "        residuals = predictions-output\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        \n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Estimating the derivative for each parameter (weight)\n",
    "            derivative = feature_derivative(residuals,feature_matrix[:, i])\n",
    "            \n",
    "            # add the squared value of the derivative to the gradient magnitude (for assessing convergence)\n",
    "            gradient_sum_squares = gradient_sum_squares + (derivative*derivative)\n",
    "            \n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i] - step_size*derivative\n",
    "        \n",
    "        # compute the square-root of the gradient sum of squares to get the gradient magnitude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        iteration = iteration+1\n",
    "\n",
    "        # The tolerance is our stoping criteria\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    \n",
    "    # Return the parameters (weights) and number of iterations\n",
    "    return(weights,iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the function over the train data\n",
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train, model_features, my_output)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9\n",
    "\n",
    "# Gradient descent\n",
    "(weights,iteration) = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "\n",
    "# Predictions over the test data\n",
    "(test_feature_matrix, test_output_new) = get_numpy_data(test, model_features, my_output)\n",
    "prediction = predict_output(test_feature_matrix, weights)\n",
    "\n",
    "# Checking outputs\n",
    "print(weights)\n",
    "print(iteration)\n",
    "print(prediction[0])\n",
    "print(output[0])\n",
    "print(prediction[0] - test_output_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
